





# 服务组件性能测试方案

 撰写人：蔡振伟     版本：v 1.00   时间：20200119

## 一、目的

1、目前CM单机版在产品和项目使用比较多，运行整体稳定，故选此作为测试对标对象。通过主机和服务的参数调配，同等硬件配置下，统一管理平台提供的组件性能指标上应不低于CM版本。

2、测试过程中，可进一步深化对影响服务组件性能的参数集的理解。形成的知识经验可进一步指导，在不同应用场景下如何配置最优参数。

## 二、方案

### 2-1、机器配置

| 项   | 主机1                                                     | 主机2                                                        | 备注 |
| ---- | --------------------------------------------------------- | ------------------------------------------------------------ | ---- |
| IP   | 10.45.157.94                                              | 10.45.157.11                                                 |      |
| CPU  | Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz*2<br />（48核） | Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz*2<br />（48核）    |      |
| 内存 | 16G 2133MHz * 8                                           | 16G 2133MHz * 8                                              |      |
| 硬盘 | 数据盘：1T SAS 7200rpm *8                                 | 系统盘：300G SAS 10500rpm * 2<br />数据盘：1T SAS 7200rpm * 8 |      |
| OS   |                                                           |                                                              |      |

```shell
#cpu信息：
cat /proc/cpuinfo
```

```shell
#内存条信息：
dmidecode | grep -A16 "Memory Device$"
```



```shell
#硬盘信息：
fdisk -l |grep Disk
smartctl --all /dev/sda 
yum install sg3_utils
sginfo -g /dev/sda #转速
```

```shell
#查看操作系统版本号
cat /etc/redhat-release
```

```shell
#查看网卡流量
watch cat /proc/net/dev
#查看网卡信息
ethtool em1
#查看网卡型号
lspci -vvv | grep Ethernet
```



###  2-2、测试工具

#### 2-2-1、Apache JMeter

#### 2-2-2、HiBench

> HiBench is a big data benchmark suite that helps evaluate different big data frameworks in terms of speed, throughput and system resource utilizations. It contains a set of Hadoop, Spark and streaming workloads, including Sort, WordCount, TeraSort, Sleep, SQL, PageRank, Nutch indexing, Bayes, Kmeans, NWeight and enhanced DFSIO, etc. It also contains several streaming workloads for Spark Streaming, Flink, Storm and Gearpump.
>
> https://github.com/Intel-bigdata/HiBench.git
>
> 

> 目前的7.0版本支持：
>
> ### Supported Hadoop/Spark/Flink/Storm/Gearpump releases:
>
> - Hadoop: Apache Hadoop 2.x, **CDH5**, HDP
> - Spark: Spark 1.6.x, Spark 2.0.x, Spark 2.1.x, Spark 2.2.x
> - Flink: 1.0.3
> - Storm: 1.0.1
> - Gearpump: 0.8.1
> - Kafka: 0.8.2.2

做些版本的扩展，可以参考如下网址：https://github.com/stanislawbartkowski/MyHiBench.git

### 2-3、服务组件测试内容

#### 2-3-1、HDFS服务组件

##### HiBench编译

编译HiBench，支持hadoopbench框架

```shell
mvn -Phadoopbench -Dspark=2.1 -Dscala=2.11 clean package
#编译好的包，打包成一个版本，便于拷贝
tar -zcvf HiBench-7.1-SNAPSHOT_2.tar.gz --exclude=*.git HiBench/
```

问题：

1、apache-hive-0.14.0-bin.tar.gz下载太慢，通过http://ftp.oleane.net/pub/apache/dist/hive/hive-0.14.0/apache-hive-0.14.0-bin.tar.gz下载到本地仓库，修改xml文件（<HiBench_Root>/hadoopbench/sql/pom.xml)

```xml
<properties>
    <!--
    <repo>http://archive.apache.org</repo>
    <file>dist/hive/hive-0.14.0/apache-hive-0.14.0-bin.tar.gz</file>
    -->
    <repo>http://10.45.157.98</repo>
    <file>software/apache/hive/0.14.0/apache-hive-0.14.0-bin.tar.gz</file>
</properties>
```

2、apache-mahout-distribution-0.11.0.tar.gz下载过慢，通过https://mirrors.huaweicloud.com/apache/mahout/0.11.0/apache-mahout-distribution-0.11.0.tar.gz下载到本地仓库，修改xml文件（<HiBench_Root>/hadoopbench/mahout/pom.xml）

```xml
<properties>
    <!--
    <repo1>http://archive.apache.org</repo1>
    <file1>dist/mahout/0.11.0/apache-mahout-distribution-0.11.0.tar.gz</file1>
    -->
    <repo1>http://10.45.157.98</repo1>
    <file1>software/apache/mahout/0.11.0/apache-mahout-distribution-0.11.0.tar.gz</file1>
    <checksum1>32bb8d9429671c651ff8233676739f1f</checksum1>
    <repo2>http://archive.cloudera.com</repo2>
    <file2>cdh5/cdh/5/mahout-0.9-cdh5.1.0.tar.gz</file2>
    <checksum2>aa953e0353ac104a22d314d15c88d78f</checksum2>
</properties>
```

3、apache-nutch-1.2-bin.tar.gz下载过慢，通过http://apache.localhost.net.ar/nutch/apache-nutch-1.2-bin.tar.gz下载到本地仓库，修改xml文件（<HiBench_Root>/hadoopbench/nutchindexing/pom.xml）

```xml
<configuration>
    <!-- <url>http://archive.apache.org/dist/nutch/apache-nutch-1.2-bin.tar.gz</url> -->
    <url>http://10.45.157.98/software/apache/nutch//apache-nutch-1.2-bin.tar.gz</url>
</configuration>
```

##### 性能测试

创建和编辑conf/hadoop.conf

```
cp conf/hadoop.conf.template conf/hadoop.conf
```

```shell
# Hadoop home
#hibench.hadoop.home     /PATH/TO/YOUR/HADOOP/ROOT
hibench.hadoop.home  /usr/lib/hadoop

# The path of hadoop executable
hibench.hadoop.executable     ${hibench.hadoop.home}/bin/hadoop

# Hadoop configraution directory
hibench.hadoop.configure.dir  ${hibench.hadoop.home}/etc/hadoop

# The root HDFS path to store HiBench data
hibench.hdfs.master       hdfs://lv118.dct-znv.com:8020/user/root


# Hadoop release provider. Supported value: apache, cdh5, hdp
hibench.hadoop.release    cdh5
```

###### 测试项一：

生成测试数据 

```shell
 bin/workloads/micro/wordcount/prepare/prepare.sh
```

运行wordcount测试例子

```shell
 bin/workloads/micro/wordcount/hadoop/run.sh
```



#### 2-3-2 Kafka组件

> 参考
>
> https://www.cnblogs.com/xiaodf/p/6023531.html
>
> https://www.infoq.cn/article/kafka-analysis-part-5
>
> https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines

使用kafka自带的测试脚本

> ### Kafka 性能测试脚本
>
> - `$KAFKA_HOME/bin/kafka-producer-perf-test.sh` 该脚本被设计用于测试 Kafka Producer 的性能，主要输出 4 项指标，总共发送消息量（以 MB 为单位），每秒发送消息量（MB/second），发送消息总数，每秒发送消息数（records/second）。除了将测试结果输出到标准输出外，该脚本还提供 CSV Reporter，即将结果以 CSV 文件的形式存储，便于在其它分析工具中使用该测试结果 
> - `$KAFKA_HOME/bin/kafka-consumer-perf-test.sh` 该脚本用于测试 Kafka Consumer 的性能，测试指标与 Producer 性能测试脚本一样。

#####  生产者测试工具

```shell
[root@lv123 bin]# ./kafka-producer-perf-test.sh 
usage: producer-performance [-h] --topic TOPIC --num-records NUM-RECORDS [--payload-delimiter PAYLOAD-DELIMITER] --throughput THROUGHPUT
                            [--producer-props PROP-NAME=PROP-VALUE [PROP-NAME=PROP-VALUE ...]] [--producer.config CONFIG-FILE]
                            [--print-metrics] [--transactional-id TRANSACTIONAL-ID] [--transaction-duration-ms TRANSACTION-DURATION]
                            (--record-size RECORD-SIZE | --payload-file PAYLOAD-FILE)

This tool is used to verify the producer performance.

optional arguments:
  -h, --help             show this help message and exit
  --topic TOPIC          produce messages to this topic
  --num-records NUM-RECORDS
                         number of messages to produce
  --payload-delimiter PAYLOAD-DELIMITER
                         provides delimiter to be used when --payload-file  is  provided.  Defaults  to new line. Note that this parameter
                         will be ignored if --payload-file is not provided. (default: \n)
  --throughput THROUGHPUT
                         throttle maximum message throughput to *approximately* THROUGHPUT messages/sec
  --producer-props PROP-NAME=PROP-VALUE [PROP-NAME=PROP-VALUE ...]
                         kafka producer  related  configuration  properties  like  bootstrap.servers,client.id  etc.  These  configs  take
                         precedence over those passed via --producer.config.
  --producer.config CONFIG-FILE
                         producer config properties file.
  --print-metrics        print out metrics at the end of the test. (default: false)
  --transactional-id TRANSACTIONAL-ID
                         The transactionalId to use if transaction-duration-ms is >  0.  Useful when testing the performance of concurrent
                         transactions. (default: performance-producer-default-transactional-id)
  --transaction-duration-ms TRANSACTION-DURATION
                         The max age of  each  transaction.  The  commitTransaction  will  be  called  after  this  this time has elapsed.
                         Transactions are only enabled if this value is positive. (default: 0)

  either --record-size or --payload-file must be specified but not both.

  --record-size RECORD-SIZE
                         message size in bytes. Note that you must provide exactly one of --record-size or --payload-file.
  --payload-file PAYLOAD-FILE
                         file to read the message payloads from. This works only  for UTF-8 encoded text files. Payloads will be read from
                         this file and a payload will be randomly selected  when  sending messages. Note that you must provide exactly one
                         of --record-size or --payload-file.
```

##### 消费者测试工具

```shell
[root@lv123 bin]# ./kafka-consumer-perf-test.sh 
Missing required argument "[topic]"
Option                                   Description                            
------                                   -----------                            
--broker-list <String: host>             REQUIRED (unless old consumer is       
                                           used): A broker list to use for      
                                           connecting if using the new consumer.
--consumer.config <String: config file>  Consumer config properties file.       
--date-format <String: date format>      The date format to use for formatting  
                                           the time field. See java.text.       
                                           SimpleDateFormat for options.        
                                           (default: yyyy-MM-dd HH:mm:ss:SSS)   
--fetch-size <Integer: size>             The amount of data to fetch in a       
                                           single request. (default: 1048576)   
--from-latest                            If the consumer does not already have  
                                           an established offset to consume     
                                           from, start with the latest message  
                                           present in the log rather than the   
                                           earliest message.                    
--group <String: gid>                    The group id to consume on. (default:  
                                           perf-consumer-30881)                 
--help                                   Print usage.                           
--hide-header                            If set, skips printing the header for  
                                           the stats                            
--messages <Long: count>                 REQUIRED: The number of messages to    
                                           send or consume                      
--new-consumer                           Use the new consumer implementation.   
                                           This is the default, so this option  
                                           is deprecated and will be removed in 
                                           a future release.                    
--num-fetch-threads <Integer: count>     Number of fetcher threads. (default: 1)
--print-metrics                          Print out the metrics. This only       
                                           applies to new consumer.             
--reporting-interval <Integer:           Interval in milliseconds at which to   
  interval_ms>                             print progress info. (default: 5000) 
--show-detailed-stats                    If set, stats are reported for each    
                                           reporting interval as configured by  
                                           reporting-interval                   
--socket-buffer-size <Integer: size>     The size of the tcp RECV size.         
                                           (default: 2097152)                   
--threads <Integer: count>               Number of processing threads.          
                                           (default: 10)                        
--topic <String: topic>                  REQUIRED: The topic to consume from.   
--zookeeper <String: urls>               REQUIRED (only when using old          
                                           consumer): The connection string for 
                                           the zookeeper connection in the form 
                                           host:port. Multiple URLS can be      
                                           given to allow fail-over. This       
                                           option is only used with the old     
                                           consumer.                                           
                                  
```

##### 测试项-创建topic

示例命令

```shell
bin/kafka-topics.sh --zookeeper esv4-hcl197.grid.linkedin.com:2181 --create --topic test-rep-one --partitions 6 --replication-factor 1
bin/kafka-topics.sh --zookeeper esv4-hcl197.grid.linkedin.com:2181 --create --topic test --partitions 6 --replication-factor 3
```



##### 测试项 -producer only

######  Producer Number VS. Throughput



###### Message Size VS. Throughput

######  Partition Number VS. Throughput

###### Replica Number VS. Throughput

##### 测试项-consumer only

##### 测试项-consumer and producer



###### 

#### 2-3-3 Spark组件

##### 修改hibench并编译

支持spark 2.3.x

sparkbench/pom.xml

```xml
    <!-- support spark 2.3 @czw-->
    <profile>
      <id>spark2.3</id>
      <properties>
        <spark.version>2.3.0</spark.version>
        <spark.bin.version>2.3</spark.bin.version>
      </properties>
      <activation>
        <property>
          <name>spark</name>
          <value>2.3</value>
        </property>
      </activation>
    </profile>
```

sparkbench/streaming/pom.xml

```xml
    <!-- support spark 2.3 @czw-->  
    <profile>
      <id>spark2.3</id>
      <dependencies>
        <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
          <version>2.3.0</version>
        </dependency>
      </dependencies>
      <activation>
        <property>
          <name>spark</name>
          <value>2.3</value>
        </property>
      </activation>
    </profile>
```

sparkbench/structuredStreaming/pom.xml

```xml
    <!-- support spark 2.3 @czw-->
    <profile>
      <id>spark2.3</id>
      <dependencies>
        <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-streaming-kafka-0-8_${scala.binary.version}</artifactId>
          <version>2.3.0</version>
        </dependency>
        <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-sql-kafka-0-10_${scala.binary.version}</artifactId>
          <version>2.3.0</version>
        </dependency>
      </dependencies>
      <activation>
        <property>
          <name>spark</name>
          <value>2.3</value>
        </property>
      </activation>
    </profile>
```

编译

```shell
mvn  -Psparkbench -Dspark=2.3 -Dscala=2.11 clean package
#编译好的包，打包成一个版本，便于拷贝
tar -zcvf HiBench-7.1-SNAPSHOT_3.tar.gz --exclude=*.git HiBench/
```

##### 配置与测试

配置`hadoop.conf`文件

```shell
cp conf/hadoop.conf.template conf/hadoop.conf
```

内容修改后

```shell
# Hadoop home
hibench.hadoop.home     /usr/lib/hadoop

# The path of hadoop executable
hibench.hadoop.executable     ${hibench.hadoop.home}/bin/hadoop

# Hadoop configraution directory
hibench.hadoop.configure.dir  ${hibench.hadoop.home}/etc/hadoop

# The root HDFS path to store HiBench data
hibench.hdfs.master       hdfs://lv123.dct-znv.com:8020/user/root


# Hadoop release provider. Supported value: apache, cdh5, hdp
hibench.hadoop.release    cdh5
```

配置Kafka

```shell
vim conf/hibench.conf
```

内容修改为

```shell
#======================================================
# Kafka for streaming benchmarks
#======================================================
hibench.streambench.kafka.home                  /home/faraday/kafka
# zookeeper host:port of kafka cluster, host1:port1,host2:port2...
hibench.streambench.zkHost lv123.dct-znv.com:2181
# Kafka broker lists, written in mode host:port,host:port,..
hibench.streambench.kafka.brokerList lv123.dct-znv.com:9092
hibench.streambench.kafka.consumerGroup          HiBench
# number of partitions of generated topic (default 20)
hibench.streambench.kafka.topicPartitions       20
```

配置Data Generator

暂时使用默认

配置spark streaming

```shell
cp conf/spark.conf.template conf/spark.conf
```

修改内容

```shell
# Spark home
hibench.spark.home      /home/faraday/spark2

# Spark master
#   standalone mode: spark://xxx:7077
#   YARN mode: yarn-client
hibench.spark.master    yarn-client
```

生成数据

```shell
#生成数据
bin/workloads/streaming/identity/prepare/genSeedDataset.sh
bin/workloads/streaming/identity/prepare/dataGen.sh

```

执行测试程序

```shell
#执行程序
bin/workloads/streaming/identity/spark/run.sh
```

运行报错

```
20/03/04 15:37:05 INFO zookeeper.ClientCnxn: EventThread shut down
Exception in thread "main" java.lang.NoSuchMethodError: kafka.admin.AdminUtils$.createTopic(Lorg/I0Itec/zkclient/ZkClient;Ljava/lang/String;IILjava/util/Properties;)V
        at com.intel.hibench.common.streaming.metrics.MetricsUtil$.createTopic(MetricsUtil.scala:39)
        at com.intel.hibench.sparkbench.streaming.RunBench$.main(RunBench.scala:66)
        at com.intel.hibench.sparkbench.streaming.RunBench.main(RunBench.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
```

解决思路：将kafka_2.11-0.8.2.1.jar 包放到spark集群上

```shell
[root@lv98-dct 0.8.2.1]# scp kafka_2.11-0.8.2.1.jar root@10.45.157.123://home/faraday/spark2/jars
kafka_2.11-0.8.2.1.jar                                                                                       100% 3862KB  73.3MB/s   00:00 
```

报错

```shell
Exception in thread "main" org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/":hdfs:supergroup:drwxr-xr-x
```

root用户对根/目录无写权限，解决方法

```shell

sudo -u hdfs hadoop fs -chmod 777 /
```

报错

```
ERROR streaming.CheckpointWriter: Could not submit checkpoint task to the thread pool executor
java.util.concurrent.RejectedExecutionException: Task org.apache.spark.streaming.CheckpointWriter
```

参考：https://github.com/Intel-bigdata/HiBench/issues/416

> Hello,
>
> I think I found the cause.
> The datanode of my HDFS is running with a HDD which is very slow.
> It incured a long processing time for checkpointing.
>
> Fix method:
>
> 1. I tried to turn off checkpointing and it worked.
> 2. I tried to set "conf/spark.conf:hibench.streambench.spark.batchInterval" to a larger value. it worked.
>
> BR
> yong

修改conf/spark.conf文件

```shell
#修改前
# Spark streaming Batchnterval in millisecond (default 100)
hibench.streambench.spark.batchInterval          100
#修改后
hibench.streambench.spark.batchInterval          1000
```

生成报告

```shell

#生成报告
bin/workloads/streaming/identity/common/metrics_reader.sh
#选择topic时，选择SPARK_identity_1_5_50_*中时间后面数字大的
```

```shell
patching args=
Parsing conf: /home/czw/test_tools/HiBench/conf/hadoop.conf
Parsing conf: /home/czw/test_tools/HiBench/conf/hibench.conf
Parsing conf: /home/czw/test_tools/HiBench/conf/spark.conf
Parsing conf: /home/czw/test_tools/HiBench/conf/workloads/streaming/identity.conf
probe sleep jar: /usr/lib/hadoop//../hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar
start MetricsReader bench
SPARK_identity_1_5_50_1583381551607
SPARK_identity_1_5_50_1583386037933
SPARK_identity_1_5_50_1583386733773
SPARK_identity_1_5_50_1583387315537
SPARK_identity_1_5_50_1583392617120
SPARK_identity_1_5_50_1583392633176
SPARK_identity_1_5_50_1583393686295
SPARK_identity_1_5_50_1583394465990
__consumer_offsets
identity
test_rep_one
Please input the topic:SPARK_identity_1_5_50_1583394465990
log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkConnection).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
log4j:WARN No appenders could be found for logger (org.I0Itec.zkclient.ZkEventThread).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Starting MetricsReader for kafka topic: SPARK_identity_1_5_50_1583394465990
Collected 1085 results for partition: 1
Collected 1085 results for partition: 0
Collected 1084 results for partition: 13
Collected 1084 results for partition: 14
Collected 1085 results for partition: 9
Collected 1085 results for partition: 18
Collected 1086 results for partition: 16
Collected 1084 results for partition: 7
Collected 1085 results for partition: 3
Collected 1085 results for partition: 6
Collected 1084 results for partition: 5
Collected 1085 results for partition: 12
Collected 1085 results for partition: 19
Collected 1085 results for partition: 8
Collected 1084 results for partition: 2
Collected 1085 results for partition: 15
Collected 1085 results for partition: 17
Collected 1084 results for partition: 4
Collected 1084 results for partition: 11
Collected 1086 results for partition: 10
written out metrics to /home/czw/test_tools/HiBench/report/SPARK_identity_1_5_50_1583394465990.csv

finish MetricsReader bench
```



####2-3-4 ES组件

> 方案参考：https://segmentfault.com/a/1190000011174694

在线安装

```shell
#直接用匹配安装的1.4.1版本需要JDK13
pip3 install esrally
#本地是JDK 8 安装1.0.2版本
pip3 install esrally==1.0.2
#需要安装python开发包
yum install python-devel.x86_64
yum install python3-devel.x86_64
#git需要1.9版本以上


```

自带tracks需要在线下载数据，数据来源亚马逊云，很慢。有人已上传到了百度网盘，地址为https://pan.baidu.com/s/123zgferlhWflOj7qJxFD1w#list/path=%2F

将自带的tracks改造下，支持数据从本地读取

数据已放在本地如下目录

```shell
cd /disk1/rally/data/
[root@lv98-dct data]# tree
.
└── geonames
    └── documents-2.json.bz2
```

修改rally.ini[<font color=red>这一步不需要</font>]

```shell
vim /root/.rally/rally.ini 
#修改前
...
[benchmarks]
local.dataset.cache = /root/.rally/benchmarks/data
...
#修改后
...
[benchmarks]
local.dataset.cache = /disk1/rally/data/
...
```



```shell
#第一步 拷贝geonames track一份
cp -r /root/.rally/benchmarks/tracks/default/geonames/* /disk1/rally/data/geonames
#修改track.json文件
#数据和track.json在一个目录下
#修改前
...
"data-url": "http://benchmarks.elasticsearch.org.s3.amazonaws.com/corpora/geonames",
  "indices": [
    {
      "name": "geonames",
      "body": "index.json",
      "types": [ "type" ]
    }
  ],
  "corpora": [
    {
      "name": "geonames",
      "base-url": "http://benchmarks.elasticsearch.org.s3.amazonaws.com/corpora/geonames",
      "documents": [
       {
          "source-file": "documents-2.json.bz2",
          "document-count": 11396503,
          "compressed-bytes": 265208777,
          "uncompressed-bytes": 3547613828
     
 ...
 
...
#修改后
...

  "indices": [
    {
      "name": "geonames",
      "body": "index.json",
      "types": [ "type" ]
    }
  ],
  "corpora": [
    {
       "name": "geonames",
       "documents": [
        {
          "source-file": "documents-2.json.bz2",
          "document-count": 11396505,
          "compressed-bytes": 264698741,
          "uncompressed-bytes": 3547614383
     
 ...
 
```



```shell
[root@lv98-dct rally-tracks]# esrally list tracks --track-path=/disk1/rally/data/geonames

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available tracks:

Name      Description         Documents    Compressed Size    Uncompressed Size    Default Challenge    All Challenges
--------  ------------------  -----------  -----------------  -------------------  -------------------  -----------------------------------------------------------------------------
geonames  POIs from Geonames  11,396,503   252.9 MB           3.3 GB               append-no-conflicts  append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts

-------------------------------
[INFO] SUCCESS (took 7 seconds)
```



```shell
#用geonames测试已部署集群,结果保存成csv文件
esrally  --pipeline=benchmark-only  --track-path=/disk1/rally/data/geonames --target-hosts=10.45.157.123:9200 --report-format=csv --report-file=~/benchmarks/157.123_geonames_test.csv
```

输出测试报告

